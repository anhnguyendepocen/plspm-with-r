% !Rnw root = ../PLS_Path_Modeling_with_R.Rnw


\chapter{Setting the PLS-PM Framework}
This is the chapter where I get to wear the hat of the tedious statistician. As you know, every statistical method has an underlying theoretical background and a conceptual framework. That's where people list the assumptions and the conditions for the models, the data, and the proposed solutions. It is certainly not the most exciting part of the book but if I skip its contents all my mentors and colleagues will probably blame me for heresy. Not that I care a lot though, but I'm sure there would be some extremely dissapointed readers for not finding the information contained in this chapter. We'll begin with a short opinionated section that talks about the underlying \textit{frame of mind} in PLS Path Modeling. Next, we'll present the formal specifications of a PLS Path Model. And then we'll discuss how the algorithm works.



\section{PLS-PM Frame of Mind}
Before describing the PLS-PM methodology, I would like to tell you some words that you won't see that often in the PLS-related literature but that I consider very important to understand the PLS-PM framework. Although most of what is here is my personal point of view, I think it reflects enough what most \textit{PLSers} feel about the principles of PLS in general, and PLS-PM in particular.

First of all, you have to understand that PLS methods are analytical tools with algorithmic origins aiming at solving models in a very practical way. PLS methods are not derived through probabilistic reasoning or numerical optimization. Instead, PLS moves away from stringent assumptions on data while maintaining a prediction-oriented focus. It is true that it doesn't rely on the classic inferential tradition ---extensively based on assumptions about variables and error distributions--- but this does not mean that PLS lacks a solid statistical basis. To assess how closely a PLS model ``fits'' the data, we use prediction error as our measure of prediction accuracy, and resampling methods for inference purposes. 

From the Structural Equation Modeling (SEM) standpoint, PLS-PM offers a different approach that doesn't impose any distributional assumptions on the data that are hard to meet in real life, especially for non-experimental data. When we use a covariance-based SEM approach we are implicitly assuming that the data is generated by some ``true'' theoretical model. In this scenario, the goal of covariance structure analysis (CSA) is to recover the ``true'' model that gave rise to the observed covariances. Briefly, when using CSA we are concerned with fitting a model and reproducing the observed covariances. This approach resorts to classical theory of statistical inference and is based on a heavy use of distributional assumptions about the behavior and personality of the data. Consequently, the analyst is forced to move slowly; and the modeling process requires careful thought and stringent justifications that more often than not end up compromising the whole analysis with the bizarre (and sometimes contradictory) principle of \textit{the data must follow the model}. 

In contrast, PLS-PM does not rely on a data-generation process and causal-modeling interpretations. 
Instead, PLS-PM treats the data ``just'' as a dataset. What I mean by this is that although there can be a data-generation process in principle, it plays no direct role in PLS-PM. The proposed models are not considered to be ground truth, but only an approximation with useful predictiveness. In other words, PLS-PM assumes no model by which the data were generated. There is only the data and nothing but the data. In this sense, PLS-PM follows the spirit of a dimension reduction technique that we can use to get useful insight of the data on hand. The ultimate goal in PLS-PM is to provide a practical summary of how the set of dependent variables are systematically explained by their sets of predictors.

Besides the description of PLS Path Modeling as an alternative approach to SEM covariance structure analysis, PLS-PM can also be regarded as a technique for analyzing a system of relationships between multiple blocks of variables, or if you want to put it in simple terms, multiple data tables. There are many statistical methods that we can use to study data presented with several blocks of variables observed on the same subjects: Hort's Generalized Canonical Correlation Analysis, Carroll's Generalized Canonical Correlation analysis, Multiple Factor Analysis, etc. The idea behind all these methods is to identify or uncover a common structure among the blocks of variables.

From the Multiple Table Analysis standpoint, PLS-PM offers a great tool for analyzing high dimensional data in a low-structure environment. In particular, PLS-PM provides the nice feature of being used as a method for prediction purposes. Given the fact that a lot of mutiple table techniques are only focused on description and exploration tasks, PLS-PM can be used for more explanatory and confirmatory purposes.

In summary, we can regard PLS-PM as a coin with the two following faces:
\begin{itemize}
 \item PLS Path Modeling as a component-based alternative for estimating Structural Equation Models.
 \item PLS Path Modeling as a method for analyzing a system of linear relationships between multiple blocks of variables.
\end{itemize}


\section{PLS Path Modeling Specs}
Behind every statistical method there is a conceptual framework that we use to formally represent the ``stuff'' that we want to model, the way in which we want to model it, what do we want to obtain, and how can we obtain it. PLS-PM is no exception and we need to talk about the \textit{behind the scenes} of this methodology. I find it useful to distinguish between the \textbf{formal model} and the \textbf{operative model}. By \textit{formal model} I mean the statement of the theoretical model, the required assumptions, and the more or less abstract notions. By \textit{operative} I mean the computational side, that is, the proposed solution. In short: the formal model is ``what we want'' while the operative model is ``how to get what we want''.


\section{Formal Model: \textit{What we want}}
Let's start with the formalization of a PLS Path Model and its specifications. These are the things that live in the abstract universe of the statistical models. You can think of the specifications as our idealized representation of how the world looks like through the PLS-PM glass. Keep in mind, however, that this is just a representation of how the world ``could'' work, not how the world ``should'' work.

Every PLS Path Model is formed by two submodels: the structural or inner model, and the measurement or outer model. The structural model is the part of the model that has to do with the relationships between the latent variables. In turn, the measurement model is the part of the model that has to do with the relationships of a latent variable with its block of manifest variables.

\subsection*{Notation}
Let's assume that we have $p$ variables measured on $n$ observations (i.e. individuals, cases, samples), and that the variables can be divided in $J$ blocks. We will use the following notation in the rest of the chapter:
\begin{itemize}
 \item[] $\mathbf{X}$ is the data set containing the $n$ observations and $p$ variables. You can think of $\mathbf{X}$ as a matrix of dimension $n \mathsf{x} p$
 
 \item[] $\mathbf{X}$ can be divided in $J$ (mutually exclusive) blocks $\mathbf{X_1}, \mathbf{X_2}, \dots, \mathbf{X_J}$

 \item[] Each block $\mathbf{X_j}$ has $K$ variables: $X_{j1}, \dots, X_{jK}$
 
 \item[] Each block $\mathbf{X_j}$ is assumed to be associated with a latent variable $LV_j$. Keep in mind that $LV_j$ is just an abstract representation (i.e. unobserved).
 
 \item[] The estimation of a latent variable, also known as score, is denoted by $\widehat{LV_j} = Y_j$
\end{itemize}


\subsection{The Structural Model}
First, let us talk about the specifications of the structural part in a PLS Path Model. There are three things that you need to consider about the inner relationships:

\paragraph{1) Linear Relationships} The first aspect of an inner model is that we treat all the structural relationships as linear relationships. We can express the structural relations in mathematical notation as:
$$ LV_j = \beta_0 + \sum_{i \rightarrow j} \beta_{ji} LV_i + error_j$$
The subscript $i$ of $LV_i$ refers to all the latent variables that are supposed to predict $LV_j$. The coefficients $\beta_{ji}$ are the \textbf{path coefficients} and they represent the ``strength and direction'' of the relations between the response $LV_j$ and the predictors $LV_i$. $\beta_{0}$ is just the intercept term, and the $error_j$ term accounts for the residuals.

\paragraph{2) Recursive Models} The second thing you need to be aware of is that the system of equations must be a \textbf{recursive} system. What this means is that the paths formed by the arrows of the inner model cannot form a loop. 

\paragraph{3) Regression Specification} The third aspect about the inner relationships is something called \textit{predictor specification} which is just a fancy term to express a linear regression idea. That's why I prefer to call it \textit{regression specification}. The idea behind this specification is that the linear relationships are conceived from a standard regression perspective:

$$ E(LV_j | LV_i) = \beta_{0i} + \sum_{i \rightarrow j} \beta_{ji} LV_i $$

What we are saying in the previous equation, in a regression sense, is that we want to understand as far as possible the conditional expected values of the response $LV_j$ determined by its predictors $LV_i$. The only extra assumption is:

$$ cov(LV_j, error_j) = 0 $$

which means that a latent variable $LV_j$ is uncorrelated with the residual $error_j$. Notice that we are assuming nothing about the distributions of the variables and error terms. We are just requiring the existence of first and second order moments in the variables.



\subsection{The Measurement Model}
Now it's time to talk about the measurement or outer model. Remember this is the part of a model that has to do with the relationships between a latent variable and its block of manifest variables. The relevant aspect about the outer model that you should bare in mind is that there are two main measurement options: reflective blocks and formative blocks. 

\paragraph{1a) Reflective Way}
The most common type of measurement is the \textit{reflective} mode. In this case the latent variable is considered as the cause of the manifest variables. That's why it's called \textit{reflective} because the manifest variables are ``reflecting'' the latent variable. If we had a latent variable $LV_1$ measured with three indicators, we could represent them with the next path diagram (note the direction of the arrows):
<<reflective_way, echo=FALSE, fig.keep='last', fig.width=2, fig.height=2, out.width='.25\\linewidth', out.height='.25\\linewidth', fig.align='center', fig.pos='h', fig.cap='Path diagram of a reflective block'>>=
# load the package
library(pathdiagram)

# define block1
block1 = list(
  mv1 = manifest(expression(x[11]), x=0.15, y=0.9, width=0.09, height=0.08, fill="gray80"),
  mv2 = manifest(expression(x[12]), x=0.15, y=0.75, width=0.09, height=0.08, fill="gray80"),
  mv3 = manifest(expression(x[13]), x=0.15, y=0.6, width=0.09, height=0.08, fill="gray80"))
LV1 = latent(expression(LV[1]), x=0.35, y=0.75, rx=0.08, ry=0.06, fill="gray50")

# PLOT
op = par(mar = rep(0,4))
wall(xlim=c(.1,.45), ylim=c(0.55, 0.95))
# draw latent variables
draw(LV1)
# draw manifest variables
for (i in 1:3) {
  draw(block1[[i]])
  arrow(from=LV1, to=block1[[i]], start="west", end="east", col="gray85", length=.20, angle=15)
}
par(op)
@

\paragraph{1b) Formative Way}
The other type of measurement is the \textit{formative} mode. In this case the manifest variables are considered to be the cause of the latent variable. That's why it's called \textit{formative} because the manifest variables are ``forming'' the latent variable. If we had a latent variable $LV_2$ measured with three indicators, we could represent them with the next path diagram (note the direction of the arrows):
<<formative_way, echo=FALSE, fig.keep='last', fig.width=2, fig.height=2, out.width='.25\\linewidth', out.height='.25\\linewidth', fig.align='center', fig.pos='h', fig.cap='Path diagram of a formative block'>>=
# define block2
block2 = list(
  mv4 = manifest(expression(x[21]), x=0.15, y=0.4, width=0.09, height=0.08, fill="gray80"),
  mv5 = manifest(expression(x[22]), x=0.15, y=0.25, width=0.09, height=0.08, fill="gray80"),
  mv6 = manifest(expression(x[23]), x=0.15, y=0.1, width=0.09, height=0.08, fill="gray80"))
LV2 = latent(expression(LV[2]), x=0.35, y=0.25, rx=0.08, ry=0.06, fill="gray50")

# PLOT
op = par(mar = rep(0,4))
wall(xlim=c(.1,.45), ylim=c(0.05, 0.45))
# draw latent variables
draw(LV2)
# draw manifest variables
for (i in 1:3) {
  draw(block2[[i]])
  arrow(from=block2[[i]], to=LV2, start="east", end="west", col="gray85", length=.20, angle=12)
}
par(op)
@

\paragraph{2) Linear Relationships} Just like in the inner model, the outer model relationships are also considered to be linear. In mathematical notation, we have that:
$$ X_{jk} = \lambda_{0jk} + \lambda_{jk} LV_j + error_{jk}   \hspace{10mm}  \mathsf{Reflective}$$
$$ LV_j = \lambda_{0j} + \lambda_{jk} X_{jk} + error_j   \hspace{10mm}  \mathsf{Formative}$$
The coefficients $\lambda_{jk}$ are called \textbf{loadings}; $\lambda_{0}$ is just the intercept term, and the $error$ terms account for the residuals. Here I'm doing some notation abuse but it's to keep things simple.

\paragraph{3) Regression Specification} In addition, we also have the concept of \textit{predictor specification} or regression specification: the linear relationships are conceived from a standard regression perspective:

$$ E(X_{jk} | LV_j) = \lambda_{0jk} + \lambda_{jk} LV_j  \hspace{10mm}  \mathsf{Reflective} $$
$$ E(LV_j | X_{jk}) = \lambda_{0j} + \lambda_{jk} X_{jk}  \hspace{10mm}  \mathsf{Formative}$$

What we are saying in the previous equations in a regression sense is that we want to understand as far as possible the conditional expected values of the response variables (either manifest or latent) in terms of the predictor ones.


\subsection{The Weight Relations}
There's one last thing belonging to the specifications of a PLS Path Model that we need to mention: the \textit{weight relations}. So far we've talked about the theoretical specifications that give formal shape to the inner and the outer models. But there's one piece that is kind of in the air. Do you know what piece are we referring to? The latent variables! All the linear equations and the considered assumptions depend on the latent variables $LV_j$, but the issue is that they are virtual entities. I don't know if you remember what I said when I defined the notation but I clearly stated that: \textit{Keep in mind that $LV_j$ is just an abstract representation}. Maybe I wasn't clear enough but what I meant was that the latent variables $LV_j's$ are like ghost variables. Unless we have a way to materialize our abstract latent variables, all the previous specifications are worthless. But fear not. Thanks to the \textit{weight relations} we can bridge the gap between the virtual LVs and the material LVs. 

In PLS-PM, latent variables are estimated as a linear combination of their manifest variables. Moreover, an estimated $LV_j$ is called a \textbf{score}, which we'll denote as $Y_j$:
$$\widehat{LV_j} = Y_j = \sum_{k} w_{jk} X_{jk}$$
In fact, this is the very reason why PLS-PM is referred to as a \textit{component-based} approach because latent variables are calculated as a weighted sum of their indicators, something similar to what is done in principal component analysis.

It is important not to confuse the role that plays the abstract $LV_j$ with the role that plays the score $Y_j$. Yes, they both refer to the same construct, but while the former is used for theoretical reasons, the later is used for practical purposes. I know this can muddle you up but don't worry.  It doesn't matter if a latent variable is measured in a reflective or a formative way; a latent variable is calculated as a linear combination of its block of indicators.




\section{Operative Model: \textit{How to get what we want}}
So far we've been discussing the specifications behind any PLS Path Model. Remember that these specifications are an idealized representation of the world looked at through the PLS-PM glass. In sum, we've only talked about ``what we want''; now we need to discuss the operative side which is basically: ``how to get what we want''.

The basic idea of PLS Path Modeling is to combine the manifest variables of each block to form an estimated (proxy) latent variable. Once the latent scores are calculated, we proceed with the estimation of the path coefficients and the loadings. That's the overall route to follow. What we need is to define the way in which we are going to calculate things (the scores, the path coefficients, and the loadings). 


\subsubsection*{PLS-PM Algorithm Overview}
PLS Path Modeling follows a sequential procedure that can be divided in three major stages:
\begin{itemize}
 \item[] Stage 1: Get the weights to compute latent variable scores
 \item[] Stage 2: Estimating the path coefficients (inner model)
 \item[] Stage 3: Obtaining the loadings (outer model)
\end{itemize}

\vspace{2mm}
The first stage consists of obtaining the weights that will be used to get the scores of the latent varibles. The second stage has to do with estimating the path coefficients of the inner model. And the third stage involves the computation of the loadings (outer model). 

Of all the stages, the first one is the key part of the PLS-PM methodology. By the way, this is also the tricky part. This stage is an iterative process in which the ultimate goal is to get the weights for the famous \textit{weight relations}. This is the heart of the PLS-PM methodology and it allows us to materialize the ghostly-abstract latent variables. Once we pass the first stage, the next two stages are practically a piece of cake. The estimation of the path coefficients is just a matter of running as many least squares regressions as structural equations in the model. In turn, obtaining the loadings is just a matter of computing simple correlations. 

In its purest essence, the PLS-PM algorithm is nothing more than a series of simple and multiple ordinary least squares regressions. The slippery part comes with the multiple options for deciding what kind of regression to apply, and some extra input settings we need to consider. From my personal experience, this part is perhaps the main source of confusion and intimidation for most beginners. 


\subsection*{Auxiliary Model}
For convenience, let's use a simple model to make things a little less abstract. The complete model is illustrated in the following diagram:

<<auxiliary_model, echo=FALSE, message=FALSE>>=
# define block1
block1 = list(
  mv1 = manifest(expression(x[11]), x=0.15, y=0.9, width=0.09, height=0.08, fill="gray80"),
  mv2 = manifest(expression(x[12]), x=0.15, y=0.75, width=0.09, height=0.08, fill="gray80"),
  mv3 = manifest(expression(x[13]), x=0.15, y=0.6, width=0.09, height=0.08, fill="gray80"))
LV1 = latent(expression(LV[1]), x=0.35, y=0.75, rx=0.08, ry=0.06, fill="gray50")

# define block2
block2 = list(
  mv4 = manifest(expression(x[21]), x=0.15, y=0.4, width=0.09, height=0.08, fill="gray80"),
  mv5 = manifest(expression(x[22]), x=0.15, y=0.25, width=0.09, height=0.08, fill="gray80"),
  mv6 = manifest(expression(x[23]), x=0.15, y=0.1, width=0.09, height=0.08, fill="gray80"))
LV2 = latent(expression(LV[2]), x=0.35, y=0.25, rx=0.08, ry=0.06, fill="gray50")

# define block3
block3 = list(
  mv7 = manifest(expression(x[31]), x=0.85, y=0.65, width=0.09, height=0.08, fill="gray80"),
  mv8 = manifest(expression(x[32]), x=0.85, y=0.5, width=0.09, height=0.08, fill="gray80"),
  mv9 = manifest(expression(x[33]), x=0.85, y=0.35, width=0.09, height=0.08, fill="gray80"))
LV3 = latent(expression(LV[3]), x=0.65, y=0.5, rx=0.08, ry=0.06, fill="gray50")
@

<<auxiliary_path_diagram, echo=FALSE, fig.keep='last', fig.width=5, fig.height=4, out.width='.7\\linewidth', out.height='.5\\linewidth', fig.align='center', fig.pos='h', fig.cap='Diagram of our auxiliary model'>>=
# PLOT
op = par(mar = rep(0,4))
wall(xlim=c(.1,.9), ylim=c(0.05, 0.95))
# draw latent variables
draw(LV1)
draw(LV2)
draw(LV3)
# draw manifest variables
for (i in 1:3) {
  draw(block1[[i]])
  arrow(from=LV1, to=block1[[i]], start="west", end="east", col="gray90", angle=25)
  draw(block2[[i]])
  arrow(from=block2[[i]], to=LV2, start="east", end="west", col="gray90", angle=25)
  draw(block3[[i]])
  arrow(from=LV3, to=block3[[i]], start="east", end="west", col="gray90", angle=25)
}
# arrows of inner model
arrow(from=LV1, to=LV2, start="south", end="north", col="gray80", length=0.2)
arrow(from=LV1, to=LV3, start="east", end="west", col="gray80", length=0.2)
arrow(from=LV2, to=LV3, start="east", end="west", col="gray80", length=0.2)
#
par(op)
@


We are going to use a model with three latent variables: $LV_1$, $LV_2$, and $LV_3$. Each latent variable is associated with three manifest variables in the following form: $LV_1$ and $LV_3$ are reflective blocks:
$$ X_{1k} = \lambda_{1k} LV_1 + error \hspace{5mm} k = 1, 2, 3$$
$$ X_{3k} = \lambda_{3k} LV_3 + error \hspace{5mm} k = 1, 2, 3$$

In turn, $LV_2$ is a formative block:
$$ LV_2 = \sum_{k} \lambda_{jk} X_{2k} + error  \hspace{5mm} k = 1, 2, 3$$

For the structural relationships we are going to suppose two equations. One inner relationship in which $LV_2$ depends on $LV_1$:
$$ LV_2 = \beta_{21} LV_1 + error $$

The other inner relationship in which $LV_3$ depends on $LV_1$ and $LV_2$:
$$ LV_3 = \beta_{31} LV_1 + \beta_{32} LV_2 + error $$

Additionally, we will assume that all the variables are standardized (mean=0, var=1), in this way we get rid of the annoying constant terms $\beta_{0}'s$ and $\lambda_{0}'s$. 


\subsection{Stage 1: Iterative Process}
The first stage of the PLS-PM algorithm involves the computation of the weights to get the scores of the latent variables. Please keep in mind that our final PLS-PM destination is to get estimates of both the latent variables and the parameters (coefficients and loadings). The iterative procedure of the PLS algorithm, using technical jargon, proceeds as follows:
\vspace{2mm}
\begin{itemize}
 \item Start: Initial arbitrary outer weights (normalized to obtain standardized LVs)
 \item Step 1: Compute the external approximation of latent variables
 \item Step 2: Obtain inner weights
 \item Step 3: Compute the internal approximation of latent variables
 \item Step 4: Calculate new outer weights
 \item[] Repeat step 1 to step 4 until convergence of outer weights
\end{itemize}
\vspace{2mm}

If you are not familiar with the PLS argot, I bet you understand almost nothing from the previous pseudo-code of the iterative stage. There's a bunch of fanciful terminology such as \textit{outer weights}, \textit{normalized}, \textit{standardized}, \textit{external approximation}, \textit{inner weights}, and \textit{internal approximation}. I know all these terms may not make too much sense to you right now but you'll get them in the next pages (or so I hope).

First of all: if there's something that you need to have room in your head for is that a latent variable is calculated as a weighted sum of its manifest variables. This is the overriding principle that you should always remember. Repeat after me: \textit{latent variable scores are calculated as weighted sums of their indicators}. Good. In mathematical notation the previous sentence is expressed as:
$$\widehat{LV_j} = Y_j = \sum_{k} w_{jk} X_{jk}$$

The weights $w_{jk}$ to form the weighted average $\sum_{k} w_{jk} X_{jk}$ receive the especial name of \textit{outer weights}. Why \textit{outer}? Because there is another type of weights, called the \textit{inner weights}, that also appear in the iterative procedure. We'll talk about them in a second. The other thing that you need to know is that anything that has to do with the outer (measurement) model is referred to as \textit{external} or \textit{outside}. Likewise, anything that has to do with the inner (structural) model is referred to as \textit{internal} or \textit{inside}.


\subsubsection*{Step 0: Initial arbitrary outer weights}
We start the iterative process by assigning ``arbitrary'' values to the outer weights. To keep things simple, we can initialize all weights equal to one: $\tilde{w}_{jk}=1$. Taking our auxiliary model into account this implies creating three vectors $\mathbf{\tilde{w}}_k$ with all components equal to 1:
$$ \mathbf{\tilde{w}}_1 = (\tilde{w}_{11}=1, \tilde{w}_{12}=1, \tilde{w}_{13}=1) $$
$$ \mathbf{\tilde{w}}_2 = (\tilde{w}_{21}=1, \tilde{w}_{22}=1, \tilde{w}_{23}=1) $$
$$ \mathbf{\tilde{w}}_3 = (\tilde{w}_{31}=1, \tilde{w}_{32}=1, \tilde{w}_{33}=1) $$
Actually, the tilde weights $\tilde{w}_{jk}$ are not exactly the outer weights. We need to apply a little transformation that involves normalizing them so that the resulting scores have unit variance.

\subsubsection*{Step 1: External estimation}
Once we have the tilde weights $\mathbf{\tilde{w}}_k$, we get the \textit{external estimation} which consists of expressing a latent variable as a weighted sum of its indicators. In matrix notation we have
$$ Y_k \propto \mathbf{X}_k \mathbf{\tilde{w}}_k   \hspace{5mm}   k = 1, 2, 3  $$ 
Decomposing the formula for each LV we get the following:
$$ Y_1 \propto 1 X_{11} + 1 X_{12} + 1 X_{13} $$
$$ Y_2 \propto 1 X_{21} + 1 X_{22} + 1 X_{23} $$
$$ Y_3 \propto 1 X_{31} + 1 X_{32} + 1 X_{33} $$

As you can tell, I'm using the proportional symbol $\propto$ to indicate that each score $Y_j$ depends on its manifest variables $X_{jk}$ although there's something missing. To be precise with the formula of the scores we need to use the following equation:
$$ Y_j = \pm f_j \sum_{k} \tilde{w}_{jk} X_{jk} $$

Ok, now it seems that everything is getting fuzzy. Why is there a sign $\pm$ and what is $f_j$? The symbol $\pm$ is there to represent a weird phenomenon that may occur when calculating scores: the mysterious \textit{sign ambiguity}. The term $f_j$ is simply a scalar (a number) that makes $Y_j$ to be standardized. The phenomenon of the \textit{sign ambiguity} has to do when not all the indicators of a construct share the same correlation sign. It could be the case that instead of having all indicators positively correlated among them, there might be at least one of them that is negatively correlated. So to avoid problems deciding whether to use a positive or negative sign, we use a democratic approach to solve the \textit{sign ambiguity}. The solution is very simple: choose the sign so that the majority of the $X_{jk}$ is positively correlated with $Y_j$
$$sign\left[  \sum_{k} sign\left\{cor(X_{jk}, Y_j)\right\} \right]$$

The standardized LVs are finally expressed as:
$$ Y_j = \sum_{k} w_{jk} X_{jk} $$
where the weights $w_{jk}$ are the ``definite'' outer weights. In our example we have:
$$ Y_1 = w_{11} X_{11} + w_{12} X_{12} + w_{13} X_{13} $$
$$ Y_2 = w_{21} X_{21} + w_{22} X_{22} + w_{23} X_{23} $$
$$ Y_3 = w_{31} X_{31} + w_{32} X_{32} + w_{33} X_{33} $$


\subsubsection*{Step 2: Obtain Inner weights}
Once we have the initial scores of the latent variables, we put our focus on the inner model. Remember that the inner model takes into account only the relationships between latent variables. The goal in this step is to re-calculate scores but this time in a different way. Instead of getting the score of a latent varibale as a linear combination of its indicators, we will get a score as the linear combination of its associated latent variables. In other words, the connections among constructs in the inner model are taken into account in order to obtain a proxy of each latent variable calculated this time as a weighted aggregate of its adjacent LVs. The internal estimation of $LV_j$ denoted by $Z_j$ is defined by:
$$ Z_j = \sum_{i \longleftrightarrow j} e_{ij} Y_i $$
The double-headed arrow $\longleftrightarrow$ means that $LV_j$ is associated or connected with $LV_i$. If there is an arrow between $LV_j$ and $LV_i$ (it doesn't matter which is the dependent and which one the independent), then $LV_i$ must be taken into consideration for calculating $Z_j$. Because now we are dealing with the inner model, the weights $e_{ij}$ for this special combination are called \textit{inner weights}. 

The seemingly complication for PLS \textit{rookies} comes with the various options or \textit{schemes} that we have to obtain the inner weights  $e_{ij}$. There are three options to calculate the inner weights:
\begin{itemize}
 \item Centroid scheme
 \item Factor scheme
 \item Path scheme
\end{itemize}

\paragraph{Centroid scheme} This scheme only considers the sign direction of the correlations between an LV and its adjacent (neighboring) LVs. The inner weights are defined as:
$$ e_{ji} = \left\{ \begin{array}{rc}
  sign \left[ cor(Y_j, Y_i)\right] & LV_j, LV_i \hspace{2mm} adjacents \\
  0 & otherwise
  \end{array} \right\}
$$
This option does not consider the direction nor the strength of the paths in the structural model. So in theory, some problems may be present when a correlation is close to zero, causing a sign changing from +1 to -1 during the iterations.

\paragraph{Factor scheme} This scheme uses the correlation coefficient as the inner weight instead of using only the sign of the correlation. The inner weights are defined as:
$$ e_{ji} = \left\{ \begin{array}{rc}
  cor(Y_j, Y_i) & LV_j, LV_i \hspace{2mm} adjacents \\
  0 & otherwise
  \end{array} \right\}
$$
This scheme considers not only the sign direction but also the strength of the paths in the structural model.

\paragraph{Path scheme} In this case the LVs are divided in antecedents (predictors) and followers (predictands) depending on the cause-effects relationships between two LVs. An LV can be either a follower, if it is caused by another LV, or an antecedent if it is the cause of another LV. If $LV_i$ is a follower of $LV_j$ then the inner weight is equal to the correlation between $Y_i$ and $Y_j$. On the other hand, for the antecedents $LV_i$ of $LV_j$ the inner weights are the regression coefficient of $Y_i$ in the multiple regression of $LV_j$ on the $LV_i's$ associated to the antecedents of $LV_j$.
The path weighting scheme has the advantage of taking into account both the strength and the direction of the paths in the structural model. However, this scheme presents some problems when the LV correlation matrix is singular.

\vspace{2mm}
In practice, choosing one weighting scheme in particular over the others has little relevance on the estimation process. It has been observed that they do not influence the results significantly. However, in a more theoretical level, they are of a great importance to understand how PLS-PM can be applied to different techniques of multiple table analysis.


\subsubsection*{Step 3: Internal Approximation}
One we have the inner weights, we compute the internal estimation $Z_j$ as:
$$ Z_j = \sum_{i \longleftrightarrow j} e_{ij} Y_i $$

In our auxiliary example we would have:
$$ Z_1 = \sum_{i \longleftrightarrow 1} e_{i1} Y_i = e_{21} Y_2 + e_{31} Y_3 $$
$$ Z_2 = \sum_{i \longleftrightarrow 2} e_{i2} Y_i = e_{12} Y_1 + e_{32} Y_3 $$
$$ Z_3 = \sum_{i \longleftrightarrow 3} e_{i3} Y_i = e_{13} Y_1 + e_{23} Y_2 $$



\subsubsection*{Step 4: Updating Outer weights}
Once the inside approximation is done, the internal estimates $Z_j$ must then be considered with regard their indicators. This is done by updating the outer weights. There are basically two ways of calculating the outer weights $w_{ji}$: (1) mode A, and (2) mode B. Each mode corresponds to a different way of relating the MVs with the LVs in the theoretical model. Mode A is used when the indicators are related to their latent variable through a reflective way. Instead, mode B is preferred when indicators are associated with their latent variable in a formative way

\paragraph{Mode A} In reflective blocks (mode A) we obtain the outer weights $\tilde{w}_{jk}$ with simple regressions of each indicator $X_{j1}, X_{j2}, \dots, X_{jk}$ on their latent score $Y_j$
$$ \tilde{w}_{jk}= (Y'_j Y_j)^{-1} Y'_j X_{jk} $$

\paragraph{Mode B} In formative blocks (mode B) we obtain the vector of outer weights $\mathbf{\tilde{w}_j} $ with a multiple regression of $Y_j$ on $\mathbf{X}_j$
$$ \mathbf{\tilde{w}_j} = (\mathbf{X}'_j \mathbf{X}_j)^{-1} \mathbf{X}'_j Y_j $$


\subsubsection*{Check for convergence}
In every iteration step, say $S = 1, 2, 3, \dots, $ convergence is checked comparing the outer weights of step $S$ against the outer weights of step $S-1$. One common way to check convergence is
$|w_{jk}^{S-1} - w_{jk}^{S}| < 10^{-5}$
as a convergence criterion.



\subsection{Stage 2: Path Coefficients}
The second stage of the algorithm consists of calculating the path coefficient estimates, $\widehat{\beta_{ji}} = B_{ji}$. The structural coefficients are estimated by ordinary least squares in the multiple regression of $Y_j$ on the $Y_i$'s related with it: 
$$ Y_j = \sum_{i \longrightarrow j} \widehat{\beta_{ji}} Y_i $$
The least squares solution is:
$$ B_{ji} = (Y_i' Y_i)^{-1} Y_i' Y_j $$


\subsection{Stage 3: Loadings}
The third stage of the algorithm consists of calculating the loadings. For convenience and simplicity reasons, loadings are preferably calculated as correlations between a latent variable and its indicators
$$ \widehat{\lambda_{jk}} = cor(X_{jk}, Y_j) $$



\subsection{Wrapping up}
The goal of PLS is to obtain score values of latent variables for prediction purposes. The idea is to calculate estimates of latent variables as linear combinations of their associated indicators using a special linear combination. We look for a linear combination in such a way that the obtained latent variables take into account the relationships of the structural and the measurement models in order to maximize the explained variance of the dependent variables (both latent and observed variables). 

The core of the PLS algorithm is the calculation of the weights (for the linear combination) required to estimate the latent variables. The weights are obtained based on how the structural and the measurement model are specified. This is done by means of an iterative procedure in which two kinds of approximation for the latent variables are alternated until convergence of weight estimates. These two types of approximation, called the inside approximation and the outside approximation, have to do with the inner relations and the outer relations, respectively.

The algorithm begins with arbitrary initial weights used to calculate an outside approximation of the latent variables, that is, initial weights are given in order to approximate the LVs as linear combinations of their MVs. Then, the inner relations among LVs are considered in order to calculate the inside approximations, having the option of choosing between three possible scenarios, called weighting schemes, to perform this approximation: (1) centroid, (2) factor, and (3) path scheme. Once the inside approximations are obtained, the algorithm turns around to the outer relations when new weights are calculated considering how the indicators are related to their constructs: by mode A (reflective), or by mode B (formative). Mode A implies simple linear regressions while mode B implies multiple linear regressions. The simple and/or multiple regressions coefficients are then used as new weights for an outside approximation. The process continues iteratively until convergence of the weights is reached.

After convergence of the outer weights, and once the latent variables are estimated, the parameters of the structural and the measurement models can be obtained. The path coefficients are calculated by ordinary least squares regressions between LVs. There are as many regressions as endogenous latent variables. The loading coefficients, are also estimated by least squares regressions but taking into account the kind of mode to be used (reflective or formative).




\section{Reading List}
\begin{itemize}
 \vspace{2mm}
 \item \textbf{\textsf{PLS path modeling}} by Michel Tenenhaus, Vincenzo Esposito Vinzi, Yves-Marie Chatelin, and Carlo Lauro (2005). This paper in the journal \textit{Computational Statistics \& Data Analysis (48: 159-205)} is perhaps the mandatory reference for anyone interested in the specifications and computational aspects of PLS-PM with a recent publication.
 
\vspace{2mm}
 \item \textbf{\textsf{Partial least squares}} by Herman Wold (1985). This entry in the \textit{Encyclopedia of Statistical Sciences, Vol 6: 581-591} is one of the classic PLS-PM references for anyone who wants to check pne of the original sources of Herman Wold.

 \vspace{2mm}
 \item \textbf{\textsf{Systems under indirect observation: causality, structure, prediction. Vol II}} edited by Karl Joreskog and Herman Wold (1982). The first chapter by Herman Wold is also another classic reference dedicated to explain the \textit{Soft Modeling} methodology of PLS-PM.

 \vspace{2mm}
 \item \textbf{\textsf{Model Construction and Evaluation When Theoretical Knowledge is Scarce: Theory and Application of Partial Least Squares}} by Herman Wold (1980). This paper, in the book \textit{Evaluation of Econometric Models} (edited by Kmenta and Ramsey, pp: 47-74), is not very cited but is worth reading in order to understand Wold's perspective on model building, as well as the basic principles behind PLS Path Modeling. \\
 Available at: \\
 \texttt{\href{http://www.nber.org/chapters/c11693}{http://www.nber.org/chapters/c11693}}

 \vspace{2mm}
 \item \textbf{\textsf{Latent Variables Path Modeling with Partial Least Squares}} by Jan-Bernd Lohmoller (1989). This purple-cover book is another official reference for PLS-PM, although this is definitely not a text for people with phobia to heavy math notation. The best thing about this book is that it presents a detailed and wide-ranging account of the capabilities of PLS-PM. It covers all the statistical, modeling, algorithmic and programming aspects of the PLS methodology. The worst thing: is not user (reader)-friendly. Written in his very particular \textit{Lohmollerian} style, it can be extremely hard to decipher. It took me several readings to start grasping the notions and connecting the dots. Not an easy going text but worth it if you are planning to open new paths in the PLS field. 
 
\end{itemize}